{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TC - ARP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5vVPLp3CAjA",
        "colab_type": "text"
      },
      "source": [
        "# Amazon Review Polarity\n",
        "## Text Classification Using Machine Learning Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf3eQJCQZUHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "ff972a0c-be41-4f6a-f31b-dcb4be58f8e6"
      },
      "source": [
        "!pip install bayesian-optimization"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=4a190440d725c2d580a038fd0bc260b9f4cf0f50e890bc529671f804c969205f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoatpqE8i0Uq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3f77b2e5-7e12-4e40-f549-7aece41de0e5"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import tqdm\n",
        "import string\n",
        "import unicodedata\n",
        "import collections\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "from sklearn import naive_bayes\n",
        "from sklearn import metrics\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "SEED = 78"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iDJz2xLjEb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "8e8481cf-21aa-449c-a7b3-726ce9d3725c"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    'https://s3.amazonaws.com/fast-ai-nlp/amazon_review_polarity_csv.tgz' \\\n",
        "    -O './amazon_review_polarity_csv.tgz'\n",
        "\n",
        "!tar -xzvf './amazon_review_polarity_csv.tgz'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-19 23:39:02--  https://s3.amazonaws.com/fast-ai-nlp/amazon_review_polarity_csv.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.186.157\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.186.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 688339454 (656M) [application/x-tar]\n",
            "Saving to: ‘./amazon_review_polarity_csv.tgz’\n",
            "\n",
            "./amazon_review_pol 100%[===================>] 656.45M  48.2MB/s    in 12s     \n",
            "\n",
            "2020-01-19 23:39:15 (52.6 MB/s) - ‘./amazon_review_polarity_csv.tgz’ saved [688339454/688339454]\n",
            "\n",
            "amazon_review_polarity_csv/\n",
            "amazon_review_polarity_csv/train.csv\n",
            "amazon_review_polarity_csv/readme.txt\n",
            "amazon_review_polarity_csv/test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgjj0dWWjGO8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "6da52823-5221-4662-be33-8d781bff4124"
      },
      "source": [
        "%%time\n",
        "train = pd.read_csv('amazon_review_polarity_csv/train.csv', header=None)\n",
        "test = pd.read_csv('amazon_review_polarity_csv/test.csv', header=None)\n",
        "print(f'Train shape: {train.shape} - Test shape: {test.shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (3600000, 3) - Test shape: (400000, 3)\n",
            "CPU times: user 16.6 s, sys: 1.27 s, total: 17.9 s\n",
            "Wall time: 17.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj-tha8E7ypj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "4cda5245-5b6a-404d-c4fe-39de4e1be71c"
      },
      "source": [
        "train.columns = ['label', 'review_title', 'review_text']\n",
        "test.columns = ['label', 'review_title', 'review_text']\n",
        "train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review_title</th>\n",
              "      <th>review_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Stuning even for the non-gamer</td>\n",
              "      <td>This sound track was beautiful! It paints the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The best soundtrack ever to anything.</td>\n",
              "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Amazing!</td>\n",
              "      <td>This soundtrack is my favorite music of all ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>Excellent Soundtrack</td>\n",
              "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
              "      <td>If you've played the game, you know how divine...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                        review_text\n",
              "0      2  ...  This sound track was beautiful! It paints the ...\n",
              "1      2  ...  I'm reading a lot of reviews saying that this ...\n",
              "2      2  ...  This soundtrack is my favorite music of all ti...\n",
              "3      2  ...  I truly like this soundtrack and I enjoy video...\n",
              "4      2  ...  If you've played the game, you know how divine...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPjB0q3hhgH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "287505b2-a012-429f-b92e-1aa16ef4ed65"
      },
      "source": [
        "print(train.label.value_counts())\n",
        "print(test.label.value_counts())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2    1800000\n",
            "1    1800000\n",
            "Name: label, dtype: int64\n",
            "2    200000\n",
            "1    200000\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJK566Oq-WA8",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PijVM4ex79t_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"[^a-z1-9'-]\", r' ', text)\n",
        "    text = re.sub(r'\\s+', r' ', text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text, tokenizer=tokenizer, stopwords=STOPWORDS):\n",
        "    tokens = [word for word in tokenizer.tokenize(text)\n",
        "                if word not in stopwords]\n",
        "    return tokens\n",
        "\n",
        "def stem(tokens, stemmer=stemmer):\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "def lemmatize(tokens, lemmatizer=lemmatizer):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJdvNLsRTdl6",
        "colab_type": "text"
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvNHIZYtUELF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.index2word = {}\n",
        "        self.word2count = {}\n",
        "        self.num_words = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word in self.word2count.keys():\n",
        "            self.word2count[word] += 1\n",
        "        else:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.word2count[word] = 1\n",
        "            self.num_words +=1\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split():\n",
        "            self.add_word(word)\n",
        "\n",
        "    def trim(self, min_count):\n",
        "        '''Remove words below a certain count threshold'''\n",
        "        keep_words = []\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "        \n",
        "        print(f'Keeping {len(keep_words)} over {len(self.word2index)} '\n",
        "                f'=> {(len(keep_words) / len(self.word2index) * 100):.2f}%')\n",
        "        \n",
        "        self.word2index = {}\n",
        "        self.index2word = {}\n",
        "        self.word2count = {}\n",
        "        self.num_words = 0\n",
        "        \n",
        "        for word in keep_words:\n",
        "            self.add_word(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDYyPlsyQ8hT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bag_of_words(tokens, vocabulary: Vocabulary):\n",
        "    bow = np.zeros(vocabulary.num_words)\n",
        "    for token in tokens:\n",
        "        if token in vocabulary.word2index:\n",
        "            bow[vocabulary.word2index[token]] += 1\n",
        "    return bow\n",
        "\n",
        "def get_sparse_matrix(bows):\n",
        "    return sp.sparse.vstack([sp.sparse.csr_matrix(bow) for bow in bows])\n",
        "\n",
        "def train_tf_idf(train_data, tokenizer=tokenizer):\n",
        "    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenizer,\n",
        "                                       min_df=5, max_df=0.9,\n",
        "                                       ngram_range=(1, 3))\n",
        "    tfidf_vectorizer.fit(train_data)\n",
        "    return tfidf_vectorizer\n",
        "\n",
        "def get_tf_idf(vectorizer, train_data, test_data):\n",
        "    return vectorizer.transform(train_data), vectorizer.transform(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt-HrAbyY99e",
        "colab_type": "text"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5MrYw40YOSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bayesian_optimization(dataset, function, parameters):\n",
        "    X_tr, y_tr, X_te, y_te = dataset\n",
        "    n_iterations = 25\n",
        "    gp_params = {'alpha': 1e-4}\n",
        "\n",
        "    optimizer = BayesianOptimization(function, parameters)\n",
        "    optimizer.maximize(n_iter=n_iterations, **gp_params)\n",
        "\n",
        "    return optimizer.max\n",
        "\n",
        "def train_model(data, function, parameters, clf):\n",
        "    X_tr, y_tr, X_te, y_te = dataset\n",
        "    best_solution = bayesian_optimization(dataset, function, parameters)      \n",
        "    params = best_solution['params']\n",
        "\n",
        "    assert callable(clf), 'Error! clf must be a callable!'\n",
        "    model = clf(**params)\n",
        "    model.fit(X_tr, y_tr)\n",
        "\n",
        "    return model\n",
        "\n",
        "def optimize(cv_splits):\n",
        "    def function(clf, params):\n",
        "        return model_selection.cross_val_score(clf(**params), X_tr, y_tr,\n",
        "                                               cv=cv_splits,\n",
        "                                               scoring='accuracy',\n",
        "                                               n_jobs=-1).mean()\n",
        "    return function, parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mrgzy17aUV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a898be52-b36e-472c-94ea-d84a52f9d574"
      },
      "source": [
        "callable(svm.SVC)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye9q9w5VaXWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}